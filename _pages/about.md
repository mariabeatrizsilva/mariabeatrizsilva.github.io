---
layout: single
title: "About"
permalink: /about/
author_profile: true
classes: wide
---
<div style="margin-top: -1rem;">


<div style="margin-bottom: 3rem; margin-top: 1.5rem;">

<h2> Research Motivation </h2>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
I'm motivated by a fundamental gap in how we develop visual technologies: we build increasingly sophisticated systems without deeply understanding how humans actually perceive and interact with them, or whether they truly serve users' needs. My work bridges this gap by grounding graphics and AI systems in perceptual principles and human-centered considerations.
</p>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
The questions that drive me: How do we design rendering algorithms that align with human visual perception rather than abstract technical metrics? How can we build visual systems that are designed for the diverse ways people actually see and interact with technology?
</p>

</div>

<h2> Current Research </h2>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
<strong>Perceptually-Guided Temporal Antialiasing</strong> (Honors Thesis): I'm investigating the relationship between temporal antialiasing (TAA) parameters and perceptual quality metrics. TAA is widely used in real-time rendering, but parameter tuning typically relies on heuristics rather than perceptual grounding. This work aims to find a relationship between TAA parameters and perceptual metrics, with the ultimate goal of optimizing TAA to directly target perceptual quality improvements.
</p>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">

<strong>GeneVA Dataset Project</strong>: Text-to-video diffusion models produce unique spatio-temporal artifacts—flickering, temporal inconsistencies, motion distortions—that don't appear in static images. Yet we lack human-annotated datasets to systematically evaluate these artifacts. We created GeneVA to fill this gap. I built the end-to-end data collection pipeline, conducted literature reviews to inform our artifact taxonomy, and contributed to paper writing. This work addresses a critical need as video generation becomes increasingly prevalent.
</p>

<h2> Publications </h2>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
<i>Kang, J., <strong>Silva, M.B.</strong>, Sangkloy, P., Chen, K., Williams, N., & Sun, Q. </i>(2025). GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts. Accepted to WACV 2026.
</p>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
<i>Silva, C., Piadyk, Y., Rulff, J., Panozzo, D., <strong>Silva, M.B.</strong>, et al. </i>(2024). PaleoScan: Low-Cost Easy-to-use High-Volume Fossil Scanning. In *Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems* (pp. 1-16).
</p>

<h2>Teaching & Mentorship</h2>

<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
As an Education Fellow for Emerging Leaders in Technology and Engineering (ELiTE), I taught C++ and Arduino programming to high school students from underrepresented backgrounds. I continue to mentor former students as they navigate their academic paths—one mentee recently completed computer graphics research at a summer program at MIT, and I've advised others as they navigate college.
</p>
<p style="margin-left: 1.5rem; margin-bottom: 1rem; font-size: 0.95rem;">
As someone who has benefitted greatly from mentorship myself, particularly through programs like Google CSRMP and the MLT Career Preparation Program, I'd like to thank all of my previous mentors for their support. 
</p>
</div>

<!-- Outside of research, you'll find me exploring NYC's art museums and learning new languages (currently working on Italian!) -->
